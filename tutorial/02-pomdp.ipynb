{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially Observable Markov Decision Processes (POMDPs)\n",
    "\n",
    "## Introduction \n",
    "\n",
    "Partially Observable Markov Decision Processes (POMDPs) generalize Markov Decision Processes (MDPs) to scenarios where the agent does not have full observability of the environment's state. In contrast to MDPs, where the current state provides all necessary information for optimal decision-making, POMDPs introduce an observation model that captures the probabilistic relationship between the true state and the agent's observations.\n",
    "\n",
    "Formally, a POMDP extends the MDP tuple $(S, s_0, A, P, R, \\gamma)$ with an observation space $\\Omega$ and an observation model $O(z \\mid s, a)$, where $z \\in \\Omega$ is an observation, and $O(z \\mid s, a)$ represents the probability of observing $z$ after taking action $a$ in state $s$.\n",
    "\n",
    "A key concept in POMDPs is the *belief state* $b$, which represents the probability distribution over all possible states given the agent's history of observations and actions. Instead of acting directly on the hidden state, the agent maintains and updates its belief state based on its interactions with the environment.\n",
    "\n",
    "While numerous resources cover POMDPs in detail, I will highlight a few key challenges:\n",
    "\n",
    "*   **Aliasing and Noise:** Observations can be ambiguous; the same observation can arise from different states, and a single state can lead to different observations.\n",
    "*   **Scale:** The belief space is continuous and often high-dimensional, making it computationally challenging to represent and manipulate. Furthermore, the observation space can also be very large, requiring us to account for all possible observations.\n",
    "*   **Information-Gathering Actions:** A crucial aspect of POMDPs is the trade-off between exploiting current knowledge to maximize immediate rewards and taking actions that reduce state uncertainty (note these may need to occur after training, unlike most MDP approaches), potentially leading to higher long-term rewards. This leads to the concept of information-gathering actions.\n",
    "\n",
    "It is worth noting that some high-performing deep reinforcement learning algorithms address partial observability by providing the agent with sufficient context, such as stacking observations or using recurrent memory. This allows them to approximate the POMDP as an MDP. While this approach can be effective, it may not explicitly account for uncertainty and information-gathering actions in the same way that dedicated POMDP solvers do.\n",
    "\n",
    "## Notebook Objectives\n",
    "- Formalize our environment as a POMDP\n",
    "- Explore the transition model and discuss potential issues related to scale.\n",
    "- Introduce algorithms for solving POMDPs:\n",
    "\t- Q-MDP\n",
    "\t- Fast Informed Bound\n",
    "\t- Randomized Point-Based Value Iteration (aka Perseus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
